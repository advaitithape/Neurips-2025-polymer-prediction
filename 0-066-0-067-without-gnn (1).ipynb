{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Title and summary ","metadata":{}},{"cell_type":"markdown","source":"NeurIPS 2025 Polymer Prediction — Non-GNN Baselines with Stacking and Blending\n\nThis notebook builds two strong non-GNN pipelines using RDKit descriptors and 2048-bit Morgan fingerprints, trains LGBM/XGBoost/CatBoost with KFold OOF, stacks with ElasticNetCV, and blends two pipelines 50/50, followed by light post-processing (Tg mean matching). It produces submission.csv.","metadata":{}},{"cell_type":"markdown","source":"# Setup and configuration ","metadata":{}},{"cell_type":"code","source":"# --- Cell 1: Setup and Configuration ---\n\n# General data handling and modeling\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom tqdm.auto import tqdm\n\n# Scikit-learn for modeling and feature engineering\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# RDKit for chemistry features\nfrom rdkit import Chem, DataStructs\nfrom rdkit.Chem import Descriptors, AllChem\n\n# Gradient Boosting Models\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n# --- Global Configuration ---\nSEED = 42\nnp.random.seed(SEED)\ntargets = [\"Tg\", \"FFV\", \"Rg\", \"Density\", \"Tc\"]\n\n# --- The Proven Hyperparameters (identical for both pipelines) ---\noptuna_results = {'Tg': {'depth': 4, 'learning_rate': 0.0138, 'iterations': 909, 'l2_leaf_reg': 0.0029},'FFV': {'depth': 6, 'learning_rate': 0.2792, 'iterations': 319, 'l2_leaf_reg': 7.5440},'Rg': {'depth': 4, 'learning_rate': 0.0429, 'iterations': 548, 'l2_leaf_reg': 0.0342},'Density': {'depth': 10, 'learning_rate': 0.0626, 'iterations': 272, 'l2_leaf_reg': 0.8955},'Tc': {'depth': 7, 'learning_rate': 0.0281, 'iterations': 386, 'l2_leaf_reg': 0.0038}}\nlgbm_results = {'Tg': {'num_leaves': 127, 'learning_rate': 0.0126, 'n_estimators': 177, 'max_depth': 12, 'reg_alpha': 0.0011, 'reg_lambda': 0.0243},'FFV': {'num_leaves': 65, 'learning_rate': 0.0694, 'n_estimators': 983, 'max_depth': 15, 'reg_alpha': 0.0218, 'reg_lambda': 0.0020},'Rg': {'num_leaves': 136, 'learning_rate': 0.0151, 'n_estimators': 297, 'max_depth': 4, 'reg_alpha': 8.5566, 'reg_lambda': 0.0121},'Density': {'num_leaves': 96, 'learning_rate': 0.0126, 'n_estimators': 180, 'max_depth': 8, 'reg_alpha': 0.0456, 'reg_lambda': 1.4955},'Tc': {'num_leaves': 109, 'learning_rate': 0.0268, 'n_estimators': 699, 'max_depth': 6, 'reg_alpha': 0.1622, 'reg_lambda': 0.3150}}\nxgb_results = {'Tg': {'max_depth': 3, 'learning_rate': 0.0100, 'n_estimators': 668, 'reg_alpha': 0.0014, 'reg_lambda': 0.9604},'FFV': {'max_depth': 7, 'learning_rate': 0.0454, 'n_estimators': 942, 'reg_alpha': 0.0057, 'reg_lambda': 6.3910},'Rg': {'max_depth': 13, 'learning_rate': 0.0331, 'n_estimators': 906, 'reg_alpha': 8.4273, 'reg_lambda': 1.3291},'Density': {'max_depth': 3, 'learning_rate': 0.2504, 'n_estimators': 511, 'reg_alpha': 0.0114, 'reg_lambda': 1.8864},'Tc': {'max_depth': 9, 'learning_rate': 0.1164, 'n_estimators': 475, 'reg_alpha': 0.0131, 'reg_lambda': 3.4711}}\n\nprint(\"✅ Setup and configuration complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data loading and base feature generation","metadata":{}},{"cell_type":"markdown","source":"Loads competition train/test, merges official supplements (Tc/Tg/FFV), deduplicates on SMILES, and computes base features:\n\n* RDKit descriptors from Descriptors.descList.\n* 2048-bit Morgan fingerprints (radius=2).\n  \nFeatures are concatenated and cleaned with np.nan_to_num, then split back to train/test matrices.","metadata":{}},{"cell_type":"code","source":"# --- Cell 2: Data Loading and Base Feature Generation ---\n\nprint(\"--- Loading and preparing data ---\")\n# Load all data sources\ntrain_df_raw = pd.read_csv(' ')\ntest_df_raw = pd.read_csv(' ')\ndataset_tc = pd.read_csv(' ')\ndataset_tg = pd.read_csv(' ')\ndataset_ffv = pd.read_csv(' ')\n\n# Combine and deduplicate\naugmented = pd.concat([dataset_tc, dataset_tg, dataset_ffv], ignore_index=True)\ntrain_full = pd.concat([train_df_raw, augmented], ignore_index=True).drop_duplicates(subset=['SMILES']).reset_index(drop=True)\ntrain_size = len(train_full)\ncombined_df = pd.concat([train_full.drop(columns=targets), test_df_raw], ignore_index=True)\n\nprint(f\"Data loaded. Train size: {train_size}, Test size: {len(test_df_raw)}\")\n\n# Feature generation function\nprint(\"\\n--- Generating base RDKit and Morgan fingerprint features ---\")\ndescriptor_names, descriptor_funcs = zip(*Descriptors.descList)\nfp_bits = 2048\n\ndef compute_base_features(smiles_series):\n    rdkit, fp = [], []\n    for smiles in tqdm(smiles_series, desc=\"Calculating Base Features\"):\n        mol = Chem.MolFromSmiles(smiles)\n        rdkit_vals = [func(mol) if mol else np.nan for func in descriptor_funcs]\n        rdkit.append(rdkit_vals)\n        arr = np.zeros((fp_bits,), dtype=np.float32)\n        if mol: DataStructs.ConvertToNumpyArray(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=fp_bits), arr)\n        fp.append(arr)\n    return np.array(rdkit), np.array(fp)\n\nX_rdkit_all, X_fp_all = compute_base_features(combined_df['SMILES'])\n\n# Combine and clean the feature matrix\nX_all_features = np.nan_to_num(\n    np.hstack([X_rdkit_all, X_fp_all]), \n    nan=0.0, posinf=0.0, neginf=0.0\n)\nfeature_names = list(descriptor_names) + [f'FP_{i}' for i in range(fp_bits)]\nprint(f\"Base feature matrix created with shape: {X_all_features.shape}\")\n\n# Split back into training and testing sets\nX_train_all = X_all_features[:train_size]\nX_test_all = X_all_features[train_size:]\n\nprint(\"✅ Base feature generation complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature subsets per pipeline","metadata":{}},{"cell_type":"markdown","source":"Two pipelines are prepared:\n\n* Original (≈0.067): Tg/FFV use top-15 descriptors + top-15 FP bits; Rg uses its descriptors + all FP; Density and Tc use all features.\n* Interaction (≈0.066): Tg/FFV as above; Tc all features; Rg and Density are augmented with degree-2 interaction-only terms on selected descriptors.","metadata":{}},{"cell_type":"code","source":"# --- Cell 3: Pipeline-Specific Feature Subset Functions ---\n\n# Define the feature lists from the successful runs\ntop15_descriptors = {'Tg': ['MolLogP', 'TPSA', 'FractionCSP3', 'NumHDonors', 'NumValenceElectrons','NumRings', 'NumHAcceptors', 'NumRotatableBonds', 'HeavyAtomCount','NumAliphaticRings', 'NumBridgeheadAtoms', 'NumSaturatedRings','BertzCT', 'NHOHCount', 'NOCount'],'FFV': ['MolLogP', 'TPSA', 'NumHDonors', 'NumHAcceptors', 'FractionCSP3','NumRings', 'NumRotatableBonds', 'HeavyAtomCount', 'NumHeteroatoms','NumBridgeheadAtoms', 'NumSaturatedRings', 'BertzCT', 'NHOHCount', 'NOCount', 'RingCount'],'Rg': ['MolLogP', 'NumValenceElectrons', 'FractionCSP3', 'MolWt','NumRotatableBonds', 'HeavyAtomCount', 'NumHAcceptors', 'NumHDonors','NumRings', 'BertzCT', 'NumAliphaticRings', 'NumSaturatedRings','NHOHCount', 'NOCount', 'RingCount']}\ntop15_fps = {'Tg': ['FP_650', 'FP_1152', 'FP_587', 'FP_378', 'FP_1057', 'FP_1928', 'FP_80', 'FP_695','FP_1143', 'FP_891', 'FP_1911', 'FP_314', 'FP_1873', 'FP_1855', 'FP_1097'],'FFV': ['FP_587', 'FP_650', 'FP_1152', 'FP_378', 'FP_1057', 'FP_80', 'FP_695', 'FP_1928','FP_1143', 'FP_414', 'FP_891', 'FP_807', 'FP_1831', 'FP_322', 'FP_1873']}\ntop8_density_desc = ['MolWt','TPSA','HeavyAtomCount','NumHAcceptors','FractionCSP3','NumSaturatedRings','MolLogP','NumRings']\n\ndef get_feature_indices(features_to_select, all_feature_names):\n    return [i for i, f in enumerate(all_feature_names) if f in features_to_select]\n\ndef create_original_feature_subsets(X_train_all, X_test_all, feature_names):\n    \"\"\"Creates the feature subsets for the original 0.067 pipeline.\"\"\"\n    X_train_subsets, X_test_subsets = {}, {}\n    num_rdkit_features = len(descriptor_names)\n    \n    for target in ['Tg', 'FFV']:\n        selected_features = top15_descriptors[target] + top15_fps[target]\n        idxs = get_feature_indices(selected_features, feature_names)\n        X_train_subsets[target], X_test_subsets[target] = X_train_all[:, idxs], X_test_all[:, idxs]\n        \n    rg_desc_indices = get_feature_indices(top15_descriptors['Rg'], feature_names)\n    rg_fp_indices = list(range(num_rdkit_features, len(feature_names)))\n    rg_indices = sorted(list(set(rg_desc_indices + rg_fp_indices)))\n    X_train_subsets['Rg'], X_test_subsets['Rg'] = X_train_all[:, rg_indices], X_test_all[:, rg_indices]\n\n    X_train_subsets['Density'], X_test_subsets['Density'] = X_train_all, X_test_all\n    X_train_subsets['Tc'], X_test_subsets['Tc'] = X_train_all, X_test_all\n    \n    print(\"Created feature subsets for the 'Original' pipeline.\")\n    return X_train_subsets, X_test_subsets\n\ndef create_interaction_feature_subsets(X_train_all, X_test_all, feature_names):\n    \"\"\"Creates the feature subsets for the 0.066 pipeline with interactions.\"\"\"\n    X_train_subsets, X_test_subsets = {}, {}\n    num_rdkit_features = len(descriptor_names)\n\n    # Standard subsets for Tg, FFV, Tc\n    for target in ['Tg', 'FFV']:\n        selected_features = top15_descriptors[target] + top15_fps[target]\n        idxs = get_feature_indices(selected_features, feature_names)\n        X_train_subsets[target], X_test_subsets[target] = X_train_all[:, idxs], X_test_all[:, idxs]\n    X_train_subsets['Tc'], X_test_subsets['Tc'] = X_train_all, X_test_all\n\n    # Interaction features for Rg\n    rg_desc_indices = get_feature_indices(top15_descriptors['Rg'], feature_names)\n    rg_fp_indices = list(range(num_rdkit_features, len(feature_names)))\n    rg_full_indices = sorted(list(set(rg_desc_indices + rg_fp_indices)))\n    poly_rg = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False).fit(X_train_all[:, rg_desc_indices])\n    X_train_subsets['Rg'] = np.hstack([X_train_all[:, rg_full_indices], poly_rg.transform(X_train_all[:, rg_desc_indices])])\n    X_test_subsets['Rg'] = np.hstack([X_test_all[:, rg_full_indices], poly_rg.transform(X_test_all[:, rg_desc_indices])])\n\n    # Interaction features for Density\n    density_desc_indices = get_feature_indices(top8_density_desc, feature_names)\n    poly_density = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False).fit(X_train_all[:, density_desc_indices])\n    X_train_subsets['Density'] = np.hstack([X_train_all, poly_density.transform(X_train_all[:, density_desc_indices])])\n    X_test_subsets['Density'] = np.hstack([X_test_all, poly_density.transform(X_test_all[:, density_desc_indices])])\n    \n    print(\"Created feature subsets for the 'Interaction' pipeline.\")\n    return X_train_subsets, X_test_subsets\n\nprint(\"✅ Feature subset creation functions defined.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CV, OOF, and stacking","metadata":{}},{"cell_type":"markdown","source":"For each target:\n\n* Mask rows with missing labels.\n* KFold (10 or 5 splits depending on rows), train LGBM/XGB/Cat.\n* Collect OOF predictions; average test predictions across folds.\n* Train ElasticNetCV meta-learner on OOF; predict meta on test stack.","metadata":{"execution":{"iopub.status.busy":"2025-09-25T15:03:38.979559Z","iopub.status.idle":"2025-09-25T15:03:38.979818Z","shell.execute_reply.started":"2025-09-25T15:03:38.979695Z","shell.execute_reply":"2025-09-25T15:03:38.979706Z"}}},{"cell_type":"code","source":"# --- Cell 4: Training, OOF, and Stacking ---\n\ndef train_and_predict_pipeline(X_train_subsets, X_test_subsets, train_labels, test_scaffold):\n    \"\"\"Trains a full stacking ensemble and returns test predictions.\"\"\"\n    pipeline_test_preds = pd.DataFrame(index=test_scaffold.index, columns=targets)\n\n    for target in tqdm(targets, desc=\"Training Pipeline\"):\n        X_tr = X_train_subsets[target]\n        X_te = X_test_subsets[target]\n\n        # Mask to drop NaN targets\n        y_mask = ~train_labels[target].isna()\n        X_tr_masked, y_tr_masked = X_tr[y_mask], train_labels.loc[y_mask, target].values\n        \n        num_train_samples = X_tr_masked.shape[0]\n        n_test_samples = X_te.shape[0]\n        n_splits = 10 if num_train_samples >= 10 else 5\n\n        kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n        \n        oof_preds_lgbm = np.zeros(num_train_samples)\n        oof_preds_xgb  = np.zeros(num_train_samples)\n        oof_preds_cat  = np.zeros(num_train_samples)\n        test_preds_lgbm = np.zeros(n_test_samples)\n        test_preds_xgb  = np.zeros(n_test_samples)\n        test_preds_cat  = np.zeros(n_test_samples)\n\n        for fold, (train_idx, val_idx) in enumerate(kf.split(X_tr_masked)):\n            X_train, y_train = X_tr_masked[train_idx], y_tr_masked[train_idx]\n            X_val = X_tr_masked[val_idx]\n            \n            # Base models\n            lgbm = LGBMRegressor(**lgbm_results[target], random_state=SEED, verbosity=-1)\n            xgb  = XGBRegressor(**xgb_results[target], random_state=SEED)\n            cat  = CatBoostRegressor(**optuna_results[target], random_seed=SEED, logging_level='Silent')\n            \n            lgbm.fit(X_train, y_train)\n            xgb.fit(X_train, y_train)\n            cat.fit(X_train, y_train)\n            \n            # Out-of-fold preds\n            oof_preds_lgbm[val_idx] = lgbm.predict(X_val)\n            oof_preds_xgb[val_idx]  = xgb.predict(X_val)\n            oof_preds_cat[val_idx]  = cat.predict(X_val)\n            \n            # Test preds (averaged)\n            test_preds_lgbm += lgbm.predict(X_te) / n_splits\n            test_preds_xgb  += xgb.predict(X_te) / n_splits\n            test_preds_cat  += cat.predict(X_te) / n_splits\n\n        # Meta model training\n        X_meta_train = np.column_stack([oof_preds_lgbm, oof_preds_xgb, oof_preds_cat])\n        X_meta_test  = np.column_stack([test_preds_lgbm, test_preds_xgb, test_preds_cat])\n        meta_model = ElasticNetCV(cv=5, random_state=SEED, max_iter=10000)\n        meta_model.fit(X_meta_train, y_tr_masked)\n        \n        pipeline_test_preds.loc[:, target] = meta_model.predict(X_meta_test)\n\n    return pipeline_test_preds\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main execution, blending, submission","metadata":{}},{"cell_type":"markdown","source":"* Build Original and Interaction feature subsets, train and predict each pipeline.\n* 50/50 blend per target.\n* Post-process: mean-match Tg to train mean to reduce bias shift.\n* Save submission.csv.","metadata":{}},{"cell_type":"code","source":"# --- Cell 5: Main Execution, Blending, and Submission ---\n\n# 1. Generate feature subsets for the Original Pipeline\nprint(\"--- Generating predictions for Original Pipeline (0.067) ---\")\nX_train_orig, X_test_orig = create_original_feature_subsets(X_train_all, X_test_all, feature_names)\npreds_original = train_and_predict_pipeline(X_train_orig, X_test_orig, train_full, test_df_raw)\nprint(\"Original pipeline predictions generated.\\n\")\n\n# 2. Generate feature subsets for the Interaction Pipeline\nprint(\"--- Generating predictions for Interaction Pipeline (0.066) ---\")\nX_train_inter, X_test_inter = create_interaction_feature_subsets(X_train_all, X_test_all, feature_names)\npreds_interaction = train_and_predict_pipeline(X_train_inter, X_test_inter, train_full, test_df_raw)\nprint(\"Interaction pipeline predictions generated.\\n\")\n\n# 3. Blend the predictions\nprint(\"--- Blending predictions from both pipelines ---\")\nsubmission = test_df_raw[['id']].copy()\nfor target in targets:\n    # Using a simple 50/50 blend\n    submission[target] = 0.5 * preds_original[target] + 0.5 * preds_interaction[target]\nprint(\"Blending complete.\")\n\n# 4. Apply final post-processing\nprint(\"\\n--- Applying post-processing (Tg mean-matching) ---\")\ntg_train_mean = train_full['Tg'].mean()\nsubmission['Tg'] += tg_train_mean - submission['Tg'].mean()\nprint(\"Post-processing for Tg complete.\")\n\n# 5. Save the final submission file\nsubmission.to_csv('submission.csv', index=False)\nprint(\"\\n✅ Final blended submission file 'submission.csv' created successfully.\")\ndisplay(submission.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Methodology","metadata":{}},{"cell_type":"markdown","source":"This notebook delivers a clean, reproducible non‑GNN solution for the NeurIPS Polymer Prediction task. The pipeline unifies deterministic chemistry features with strong tree ensembles and a light meta‑learner:\n\n1. Data and features\n\n    * Merged competition train with supplements and deduplicated on SMILES.\n\n    * Generated RDKit physico‑chemical descriptors and 2048‑bit Morgan fingerprints (radius 2).\n\n    * Cleaned features with consistent NaN/Inf handling; split back into train/test matrices.\n\n2. Two complementary pipelines\n\n    * Original (≈0.067): Tg/FFV use curated 15 descriptors + 15 FP bits; Rg uses its descriptors + all FP; Density/Tc use full features.\n\n    * Interaction (≈0.066): Same Tg/FFV setup; Tc uses full features; Rg/Density add degree‑2 interaction‑only terms on selected descriptors.\n\n3. Modeling and stacking\n\n    * For each target, trained LightGBM, XGBoost, and CatBoost with fixed, proven hyperparameters.\n\n    * Used KFold (5–10 folds) to obtain out‑of‑fold predictions, masking rows with missing labels per target.\n\n    * Trained an ElasticNetCV meta‑learner on OOF stacks; inferred on the stacked test predictions.\n\n4. Blending and post‑processing\n\n    * Blended the two pipelines 50/50 per target.\n\n    * Applied a minimal Tg mean‑matching step to reduce bias from distribution shift.\n\n    * Wrote a ready‑to‑submit CSV.\n\n5. Notes and learnings\n\n    * Interaction terms most help Rg and Density; Original pipeline is stronger for Tg/Tc.\n\n    * Stacking outperforms simple averaging with small but consistent gains.\n\n    * Mean matching stabilizes Tg on the leaderboard without heavy post‑processing.\n\n\nPlanned extensions (handled in companion notebooks):\n\n    * GNN embeddings/predictions blended with this stack for additional lift.\n\n    * Diagnostics and ablations: feature importances, residuals, and shift checks.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}