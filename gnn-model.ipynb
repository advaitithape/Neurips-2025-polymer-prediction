{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Title and summary","metadata":{}},{"cell_type":"markdown","source":"NeurIPS 2025 Polymer Prediction — GNN Head + Blend with Non‑GNN Stack\n\nThis notebook builds molecular graphs from SMILES, trains a GNN regressor for five targets, produces out‑of‑fold and test predictions, and exports them for blending with the non‑GNN stack. The goal is complementary signal: the GNN alone may trail tuned GBMs on some targets, but blending typically lifts leaderboard score.","metadata":{}},{"cell_type":"markdown","source":"# Imports and setup","metadata":{}},{"cell_type":"code","source":"# --- Cell 1: All Imports and Setup ---\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport os\nimport gc\nimport copy\n\n# PyTorch and PyG\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Module, Linear, BatchNorm1d, ReLU, Dropout, Sequential\nimport torch.optim as optim\nfrom torch.utils.data import Subset\nfrom torch_geometric.data import Data, Dataset\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GATv2Conv, global_mean_pool, global_add_pool\n\n# RDKit\nfrom rdkit import Chem, DataStructs\nfrom rdkit.Chem import Descriptors, AllChem\nfrom rdkit.Chem.rdchem import HybridizationType\n\n# ML Models and Tools\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import ElasticNetCV\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n# --- Global Configuration ---\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntargets = [\"Tg\", \"FFV\", \"Rg\", \"Density\", \"Tc\"]\n\n# --- Environment Setup ---\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    torch.cuda.manual_seed_all(SEED)\nelse:\n    device = torch.device(\"cpu\")\nprint(f\"Using device: {device}\")\nprint(\"✅ Cell 1 Complete: All setup is ready.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"markdown","source":"Loads official train/test and any supplements used; deduplicates on SMILES; prepares target matrix with per‑target masks. Ensures splits align with non‑GNN folds for consistent blending later.","metadata":{}},{"cell_type":"code","source":"# --- Cell 2: Data Loading ---\n\nDATA_ROOT = \"/kaggle/input/neurips-open-polymer-prediction-2025\"\ntrain = pd.read_csv(f\"{DATA_ROOT}/train.csv\")\ntest  = pd.read_csv(f\"{DATA_ROOT}/test.csv\")\n\n# Optional supplements (uncomment if used in your run)\nsupp1 = pd.read_csv(f\"{DATA_ROOT}/train_supplement/dataset1.csv\")\nsupp3 = pd.read_csv(f\"{DATA_ROOT}/train_supplement/dataset3.csv\")\nsupp4 = pd.read_csv(f\"{DATA_ROOT}/train_supplement/dataset4.csv\")\n\n# Merge supplements and deduplicate on SMILES\naug = pd.concat([supp1, supp3, supp4], ignore_index=True)\ntrain_full = pd.concat([train, aug], ignore_index=True)\ntrain_full = train_full.drop_duplicates(subset=[\"SMILES\"]).reset_index(drop=True)\n\ny_df = train_full[targets].copy()\nsmiles_train = train_full[\"SMILES\"].tolist()\nsmiles_test  = test[\"SMILES\"].tolist()\n\nprint(f\"Train rows (deduped): {len(train_full)} | Test rows: {len(test)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Graph featurization","metadata":{}},{"cell_type":"markdown","source":"Converts SMILES to molecular graphs:\n\n* Atom features: element one‑hot, degree, formal charge, aromaticity, hybridization, implicit Hs.\n\n* Bond features: bond type one‑hot, conjugation, ring membership.\nGraphs stored as torch_geometric Data with x, edge_index, edge_attr.","metadata":{}},{"cell_type":"code","source":"# --- Cell 3: Graph Builder Utilities ---\n\n# Minimal periodic table set; expand if needed\nELEMENTS = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\nE2I = {e:i for i,e in enumerate(ELEMENTS)}\n\ndef atom_features(atom):\n    feats = []\n    # Element one-hot (truncate/other -> all zeros except a catch-all if desired)\n    elem = [0]*len(ELEMENTS)\n    elem[E2I.get(atom.GetSymbol(), 0)] = 1\n    feats += elem\n    # Scalar/categorical encodings\n    feats += [\n        atom.GetDegree(),\n        atom.GetFormalCharge(),\n        int(atom.GetIsAromatic()),\n        atom.GetTotalNumHs(includeNeighbors=True),\n    ]\n    # Hybridization one-hot (common set)\n    hyb_list = [\n        HybridizationType.SP, HybridizationType.SP2,\n        HybridizationType.SP3, HybridizationType.SP3D\n    ]\n    hyb = [int(atom.GetHybridization()==h) for h in hyb_list]\n    feats += hyb\n    return torch.tensor(feats, dtype=torch.float)\n\ndef bond_features(bond):\n    if bond is None:\n        # For self-loops if added; keep zero vector\n        return torch.zeros(6, dtype=torch.float)\n    btype = [\n        int(bond.GetBondType().name == \"SINGLE\"),\n        int(bond.GetBondType().name == \"DOUBLE\"),\n        int(bond.GetBondType().name == \"TRIPLE\"),\n        int(bond.GetBondType().name == \"AROMATIC\"),\n    ]\n    conj  = int(bond.GetIsConjugated())\n    inrng = int(bond.IsInRing())\n    return torch.tensor(btype + [conj, inrng], dtype=torch.float)\n\ndef smiles_to_data(smiles: str):\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        # Return empty graph fallback\n        return Data(x=torch.zeros((1, len(ELEMENTS)+4+4), dtype=torch.float),\n                    edge_index=torch.zeros((2,0), dtype=torch.long),\n                    edge_attr=torch.zeros((0,6), dtype=torch.float))\n    atoms = [atom_features(a) for a in mol.GetAtoms()]\n    x = torch.stack(atoms, dim=0) if atoms else torch.zeros((1, len(ELEMENTS)+4+4))\n\n    edges_src, edges_dst, eattrs = [], [], []\n    for bond in mol.GetBonds():\n        u = bond.GetBeginAtomIdx()\n        v = bond.GetEndAtomIdx()\n        bf = bond_features(bond)\n        # Undirected as two directed edges\n        edges_src += [u, v]\n        edges_dst += [v, u]\n        eattrs += [bf, bf]\n    if len(edges_src) == 0:\n        edge_index = torch.zeros((2,0), dtype=torch.long)\n        edge_attr  = torch.zeros((0,6), dtype=torch.float)\n    else:\n        edge_index = torch.tensor([edges_src, edges_dst], dtype=torch.long)\n        edge_attr  = torch.stack(eattrs, dim=0)\n    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset and DataLoader","metadata":{}},{"cell_type":"markdown","source":"Creates pytorch‑geometric Dataset that builds graphs lazily from SMILES, caches in memory, and exposes indices mapping back to original rows. Train/test loaders use reasonable batch sizes.","metadata":{}},{"cell_type":"code","source":"# --- Cell 4: Dataset and DataLoaders ---\n\nclass SmilesGraphDataset(Dataset):\n    def __init__(self, smiles_list, y=None, transform=None, pre_transform=None):\n        super().__init__(None, transform, pre_transform)\n        self.smiles_list = smiles_list\n        self.y = y.values if isinstance(y, pd.DataFrame) else y\n\n    def len(self):\n        return len(self.smiles_list)\n\n    def get(self, idx):\n        s = self.smiles_list[idx]\n        g = smiles_to_data(s)\n        g.idx = idx\n        if self.y is not None:\n            # y shape: 5 targets with NaN possible\n            g.y = torch.tensor(self.y[idx], dtype=torch.float)\n        return g\n\nBATCH_SIZE = 128\n\nfull_train_ds = SmilesGraphDataset(smiles_train, y_df)\ntest_ds       = SmilesGraphDataset(smiles_test, None)\n\nprint(f\"Graphs: train={full_train_ds.len()} | test={test_ds.len()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GNN model","metadata":{}},{"cell_type":"markdown","source":"Defines a lightweight GATv2 encoder with edge attributes concatenation through linear mixing; global mean pooling; a shared MLP trunk; and five regression heads for multi‑task outputs.","metadata":{}},{"cell_type":"code","source":"# --- Cell 5: GNN Model Definition ---\n\nclass GATBackbone(Module):\n    def __init__(self, in_dim, edge_dim, hid=128, layers=3, heads=2, drop=0.1):\n        super().__init__()\n        self.proj_e = Linear(edge_dim, hid)\n        self.layers = torch.nn.ModuleList()\n        last_dim = in_dim\n        for _ in range(layers):\n            self.layers.append(GATv2Conv(last_dim, hid//heads, heads=heads, edge_dim=hid, dropout=drop))\n            last_dim = hid\n        self.bn = BatchNorm1d(last_dim)\n        self.act = ReLU()\n        self.drop = Dropout(drop)\n\n    def forward(self, x, edge_index, edge_attr, batch):\n        e = self.proj_e(edge_attr) if edge_attr.numel() > 0 else torch.zeros((edge_index.size(1), self.layers[0].out_channels), device=x.device)\n        h = x\n        for gat in self.layers:\n            h = gat(h, edge_index, e)\n            h = F.elu(h)\n        h = self.bn(h)\n        h = self.act(h)\n        h = self.drop(h)\n        g = global_mean_pool(h, batch)\n        return g\n\nclass GNNRegressor(Module):\n    def __init__(self, in_dim, edge_dim, hid=128, layers=3, heads=2, drop=0.1, out_tasks=5):\n        super().__init__()\n        self.backbone = GATBackbone(in_dim, edge_dim, hid, layers, heads, drop)\n        emb = hid\n        self.trunk = Sequential(\n            Linear(emb, emb), ReLU(), Dropout(drop),\n            Linear(emb, emb//2), ReLU(), Dropout(drop),\n        )\n        self.heads = torch.nn.ModuleList([Linear(emb//2, 1) for _ in range(out_tasks)])\n\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        g = self.backbone(x, edge_index, edge_attr, batch)\n        z = self.trunk(g)\n        outs = [head(z) for head in self.heads]\n        return torch.cat(outs, dim=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training utilities","metadata":{}},{"cell_type":"markdown","source":"Implements epoch loop with MSE loss per target ignoring NaNs via masks; AdamW optimizer; cosine schedule optional; early stopping on validation loss.","metadata":{}},{"cell_type":"code","source":"# --- Cell 6: Training Utilities ---\n\ndef loss_with_mask(pred, target):\n    # pred/target: [B, 5]\n    mask = ~torch.isnan(target)\n    diff = (pred[mask] - target[mask])**2\n    return diff.mean() if diff.numel() > 0 else torch.tensor(0.0, device=pred.device)\n\ndef run_epoch(model, loader, optimizer=None):\n    is_train = optimizer is not None\n    total = 0.0\n    for data in loader:\n        data = data.to(device)\n        pred = model(data)\n        tgt = data.y if hasattr(data, \"y\") else None\n        if tgt is None:\n            continue\n        loss = loss_with_mask(pred, tgt)\n        if is_train:\n            optimizer.zero_grad(set_to_none=True)\n            loss.backward()\n            optimizer.step()\n        total += loss.item() * data.num_graphs\n    return total / max(1, len(loader.dataset))\n\ndef predict_model(model, loader):\n    outs = []\n    with torch.no_grad():\n        for data in loader:\n            data = data.to(device)\n            p = model(data)\n            outs.append(p.detach().cpu().numpy())\n    return np.vstack(outs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cross‑validation and OOF ","metadata":{}},{"cell_type":"markdown","source":"Uses KFold with consistent SEED; for each fold, train on train_idx and validate on val_idx; record OOF predictions aligned to original indices; predict on test and average across folds.","metadata":{}},{"cell_type":"code","source":"# --- Cell 7: KFold Training, OOF, and Test Predictions ---\n\nFOLDS = 5\nEPOCHS = 40\nLR = 2e-3\nWD = 1e-4\n\ndef build_loader(ds, idxs, shuffle, batch_size=BATCH_SIZE):\n    subset = Subset(ds, idxs)\n    return DataLoader(subset, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\noof = np.full((len(full_train_ds), 5), np.nan, dtype=np.float32)\ntest_preds = np.zeros((len(test_ds), 5), dtype=np.float32)\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(range(len(full_train_ds)))):\n    print(f\"\\nFold {fold+1}/{FOLDS} | train={len(tr_idx)} val={len(va_idx)}\")\n    tr_loader = build_loader(full_train_ds, tr_idx, shuffle=True)\n    va_loader = build_loader(full_train_ds, va_idx, shuffle=False)\n    te_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\n    sample_data = smiles_to_data(\"CC\")  # to infer dims; safe placeholder\n    in_dim = sample_data.x.size(1)\n    edge_dim = sample_data.edge_attr.size(1) if sample_data.edge_attr.numel() > 0 else 6\n\n    model = GNNRegressor(in_dim, edge_dim, hid=160, layers=3, heads=2, drop=0.15).to(device)\n    opt = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n\n    best = 1e9\n    patience, wait = 8, 0\n    for epoch in range(1, EPOCHS+1):\n        tr_loss = run_epoch(model, tr_loader, opt)\n        va_loss = run_epoch(model, va_loader, None)\n        if va_loss < best - 1e-5:\n            best = va_loss\n            wait = 0\n            best_state = copy.deepcopy(model.state_dict())\n        else:\n            wait += 1\n        if epoch % 5 == 0:\n            print(f\"Epoch {epoch:03d} | train {tr_loss:.5f} | val {va_loss:.5f}\")\n        if wait >= patience:\n            break\n\n    model.load_state_dict(best_state)\n    # OOF\n    va_loader = build_loader(full_train_ds, va_idx, shuffle=False)\n    oof_preds = predict_model(model, va_loader)\n    oof[va_idx] = oof_preds\n\n    # Test preds\n    fold_test = predict_model(model, te_loader)\n    test_preds += fold_test / FOLDS\n\n    del model, opt, tr_loader, va_loader, te_loader\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Export predictions","metadata":{}},{"cell_type":"markdown","source":"Saves OOF and test predictions as CSVs per target for downstream blending with the non‑GNN stack.","metadata":{}},{"cell_type":"code","source":"# --- Cell 8: Export OOF and Test Predictions ---\n\noof_df = pd.DataFrame(oof, columns=targets)\noof_df.to_csv(\"gnn_oof.csv\", index=False)\n\ntest_pred_df = pd.DataFrame(test_preds, columns=targets)\ntest_pred_df.insert(0, \"id\", test[\"id\"])\ntest_pred_df.to_csv(\"gnn_test_preds.csv\", index=False)\n\nprint(\"Saved: gnn_oof.csv and gnn_test_preds.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optional: direct blend here","metadata":{}},{"cell_type":"markdown","source":"If desired, blend with the non‑GNN predictions and write a submission; otherwise, keep this notebook focused on producing GNN predictions.","metadata":{}},{"cell_type":"code","source":"# --- Cell 9 (Optional): Blend with Non-GNN Stack ---\n\n# If non-GNN predictions are available in the working directory:\nif os.path.exists(\"stack_test_preds.csv\"):\n    stack = pd.read_csv(\"stack_test_preds.csv\")  # columns: id + targets\n    gnn  = pd.read_csv(\"gnn_test_preds.csv\")\n    blend = stack.copy()\n    for t in targets:\n        # Example 0.6 non-GNN / 0.4 GNN; adjust per CV\n        blend[t] = 0.6 * stack[t].values + 0.4 * gnn[t].values\n    blend.to_csv(\"submission.csv\", index=False)\n    print(\"submission.csv written via 0.6/0.4 blend.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Methodology","metadata":{}},{"cell_type":"markdown","source":"This GNN notebook complements the non‑GNN stacks by learning graph‑level representations from SMILES:\n\n* Atom/bond featurization encodes chemistry structure; GATv2 captures local neighborhoods with edge‑aware message passing.\n\n* Multi‑task heads predict Tg, FFV, Rg, Density, Tc jointly, with NaN‑masking in loss.\n\n* 5‑fold CV provides OOF validation and robust test averaging.\n\n* Exported predictions are designed for blending with the non‑GNN stack, typically at 30–50% weight depending on target CV.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
